{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arabic GEC Pipeline\n",
                "\n",
                "This notebook implements the QALB 2015 L2 correction pipeline.\n",
                "\n",
                "**Steps:**\n",
                "1.  Data Download & Decompression\n",
                "2.  M2 Format Parsing\n",
                "3.  Fine-tuning AraT5\n",
                "4.  Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "!pip install transformers datasets pyarabic gdown sentencepiece"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gdown\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "# Download the file from Google Drive\n",
                "file_id = '1hvLiiMvvubyCEAZK4KIWgu7qHBNCHOp-'\n",
                "url = f'https://drive.google.com/uc?id={file_id}'\n",
                "output_file = 'qalb_dataset.zip'\n",
                "\n",
                "# Only download if not exists\n",
                "if not os.path.exists(output_file):\n",
                "    gdown.download(url, output_file, quiet=False)\n",
                "\n",
                "# Unzip the file\n",
                "if os.path.exists(output_file):\n",
                "    with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
                "        zip_ref.extractall(\"data\")\n",
                "    print(\"Dataset extracted to 'data' directory.\")\n",
                "else:\n",
                "    print(\"Download failed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Step 1: The M2 Parser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import csv\n",
                "\n",
                "def parse_m2_and_generate_csv(m2_path, output_csv_path):\n",
                "    print(f\"Processing {m2_path}...\")\n",
                "    if not os.path.exists(m2_path):\n",
                "        print(f\"File not found: {m2_path}\")\n",
                "        return\n",
                "\n",
                "    sentences = []\n",
                "    with open(m2_path, 'r', encoding='utf-8') as f:\n",
                "        m2_data = f.read().strip().split(\"\\n\\n\")\n",
                "\n",
                "    processed_data = []\n",
                "\n",
                "    for entry in m2_data:\n",
                "        lines = entry.split(\"\\n\")\n",
                "        if not lines:\n",
                "            continue\n",
                "        \n",
                "        # The first line starts with 'S' and contains the original sentence (tokenized)\n",
                "        source_line = lines[0]\n",
                "        if not source_line.startswith(\"S \"):\n",
                "            continue\n",
                "            \n",
                "        original_tokens = source_line[2:].split()\n",
                "        edits = []\n",
                "        \n",
                "        # Subsequent lines start with 'A' and contain edits\n",
                "        for line in lines[1:]:\n",
                "            if line.startswith(\"A \"):\n",
                "                parts = line[2:].split(\"||\")\n",
                "                # Format: A start_off end_off||type||correction||... \n",
                "                span = parts[0].split()\n",
                "                start_off = int(span[0])\n",
                "                end_off = int(span[1])\n",
                "                correction = parts[2]\n",
                "                edits.append((start_off, end_off, correction))\n",
                "        \n",
                "        # Critical Reversal Logic: Sort edits by start_off in descending order\n",
                "        # This prevents index shifting when modifying the token list\n",
                "        edits.sort(key=lambda x: x[0], reverse=True)\n",
                "        \n",
                "        corrected_tokens = list(original_tokens)\n",
                "        for start, end, subst in edits:\n",
                "            # Python slice replacement: list[start:end] = [new_tokens]\n",
                "            # Determine if replacement is empty (deletion) or has content\n",
                "            if subst == \"-NONE-\":\n",
                "                replacement = []\n",
                "            else:\n",
                "                replacement = subst.split()\n",
                "            \n",
                "            corrected_tokens[start:end] = replacement\n",
                "            \n",
                "        original_sent = \" \".join(original_tokens)\n",
                "        corrected_sent = \" \".join(corrected_tokens)\n",
                "        \n",
                "        processed_data.append([original_sent, corrected_sent])\n",
                "\n",
                "    # Save to CSV\n",
                "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
                "        writer = csv.writer(f)\n",
                "        writer.writerow([\"incorrect\", \"correct\"])\n",
                "        writer.writerows(processed_data)\n",
                "    \n",
                "    print(f\"Saved {len(processed_data)} pairs to {output_csv_path}\")\n",
                "\n",
                "# Example usage (adjust paths after extraction)\n",
                "# Find M2 files in data folder\n",
                "# for root, dirs, files in os.walk(\"data\"):\n",
                "#     for file in files:\n",
                "#         if file.endswith(\".m2\") and \"Dev\" in file:\n",
                "#              parse_m2_and_generate_csv(os.path.join(root, file), \"qalb_full_gec.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Step 2: Model Training (AraT5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "from datasets import load_dataset\n",
                "import pyarabic.araby as araby\n",
                "\n",
                "def run_training_step():\n",
                "    if not os.path.exists('qalb_full_gec.csv'):\n",
                "        print(\"Training data 'qalb_full_gec.csv' not found. Please run Step 1 Parser first.\")\n",
                "        return\n",
                "\n",
                "    # --- Data Loading ---\n",
                "    # Load the CSV generated in Step 1\n",
                "    dataset = load_dataset('csv', data_files='qalb_full_gec.csv')\n",
                "    \n",
                "    # Split into train/validation (simple split for demo)\n",
                "    dataset = dataset['train'].train_test_split(test_size=0.1)\n",
                "    \n",
                "    model_name = \"aubmindlab/arat5-v2-base\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "    # --- Preprocessing ---\n",
                "    prefix = \"gec_arabic: \"\n",
                "    max_input_length = 128\n",
                "    max_target_length = 128\n",
                "\n",
                "    def preprocess_function(examples):\n",
                "        inputs = [prefix + araby.normalize_hamza(ex if ex else \"\") for ex in examples[\"incorrect\"]]\n",
                "        targets = [araby.normalize_hamza(ex if ex else \"\") for ex in examples[\"correct\"]]\n",
                "        \n",
                "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
                "        # Setup the tokenizer for targets\n",
                "        with tokenizer.as_target_tokenizer():\n",
                "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
                "\n",
                "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "        return model_inputs\n",
                "\n",
                "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
                "\n",
                "    # --- Training Config ---\n",
                "    batch_size = 16\n",
                "    args = Seq2SeqTrainingArguments(\n",
                "        \"arat5-gec-checkpoints\",\n",
                "        evaluation_strategy = \"steps\",\n",
                "        learning_rate=3e-5,\n",
                "        per_device_train_batch_size=batch_size,\n",
                "        per_device_eval_batch_size=batch_size,\n",
                "        weight_decay=0.01,\n",
                "        save_total_limit=3,\n",
                "        num_train_epochs=5,\n",
                "        predict_with_generate=True,\n",
                "        fp16=True, # faster training on GPU\n",
                "        push_to_hub=False,\n",
                "    )\n",
                "\n",
                "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "    # --- Metric (Simplified GLEU/CER placeholder) ---\n",
                "    def compute_metrics(eval_preds):\n",
                "        import numpy as np\n",
                "        preds, labels = eval_preds\n",
                "        # simple decoding for sanity check\n",
                "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "        # Replace -100 in the labels as we can't decode them.\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "        \n",
                "        return {\"sample_pred\": decoded_preds[0], \"sample_label\": decoded_labels[0]}\n",
                "\n",
                "    trainer = Seq2SeqTrainer(\n",
                "        model,\n",
                "        args,\n",
                "        train_dataset=tokenized_datasets[\"train\"],\n",
                "        eval_dataset=tokenized_datasets[\"test\"],\n",
                "        data_collator=data_collator,\n",
                "        tokenizer=tokenizer,\n",
                "        compute_metrics=compute_metrics\n",
                "    )\n",
                "\n",
                "    trainer.train()\n",
                "    \n",
                "    # Save the final model\n",
                "    model.save_pretrained(\"arat5-gec-finetuned\")\n",
                "    tokenizer.save_pretrained(\"arat5-gec-finetuned\")\n",
                "\n",
                "# Uncomment to run training\n",
                "# run_training_step()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Step 3: Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_inference(input_sentence):\n",
                "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "    import torch\n",
                "\n",
                "    model_path = \"arat5-gec-finetuned\"\n",
                "    \n",
                "    # Fallback if model isn't trained yet for testing this cell\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
                "    except:\n",
                "        print(\"Finetuned model not found, loading base model for demo...\")\n",
                "        model_name = \"aubmindlab/arat5-v2-base\"\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "    \n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    model = model.to(device)\n",
                "\n",
                "    # Preprocessing\n",
                "    prefix = \"gec_arabic: \"\n",
                "    text = prefix + input_sentence\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
                "\n",
                "    # Generation\n",
                "    outputs = model.generate(\n",
                "        inputs[\"input_ids\"],\n",
                "        max_length=128,\n",
                "        num_beams=5,\n",
                "        early_stopping=True,\n",
                "        no_repeat_ngram_size=2\n",
                "    )\n",
                "\n",
                "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    return corrected_sentence\n",
                "\n",
                "# Example Output Test\n",
                "test_sentence = \"ذهب الولد الى مدرسة\"\n",
                "print(f\"Original: {test_sentence}\")\n",
                "print(f\"Corrected: {run_inference(test_sentence)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}