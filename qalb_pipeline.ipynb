{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arabic GEC Pipeline\n",
                "\n",
                "This notebook implements the QALB 2015 L2 correction pipeline.\n",
                "\n",
                "**Steps:**\n",
                "1.  Data Download & Decompression\n",
                "2.  M2 Format Parsing\n",
                "3.  Fine-tuning AraT5\n",
                "4.  Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
                        "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
                        "Requirement already satisfied: pyarabic in /usr/local/lib/python3.12/dist-packages (0.6.15)\n",
                        "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
                        "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
                        "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
                        "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
                        "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
                        "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from pyarabic) (1.17.0)\n",
                        "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
                        "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
                        "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
                    ]
                }
            ],
            "source": [
                "# Install necessary libraries\n",
                "!pip install transformers datasets pyarabic gdown sentencepiece"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset extracted to 'data' directory.\n"
                    ]
                }
            ],
            "source": [
                "import gdown\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "# Download the file from Google Drive\n",
                "file_id = '1hvLiiMvvubyCEAZK4KIWgu7qHBNCHOp-'\n",
                "url = f'https://drive.google.com/uc?id={file_id}'\n",
                "output_file = 'qalb_dataset.zip'\n",
                "\n",
                "# Only download if not exists\n",
                "if not os.path.exists(output_file):\n",
                "    gdown.download(url, output_file, quiet=False)\n",
                "\n",
                "# Unzip the file\n",
                "if os.path.exists(output_file):\n",
                "    with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
                "        zip_ref.extractall(\"data\")\n",
                "    print(\"Dataset extracted to 'data' directory.\")\n",
                "else:\n",
                "    print(\"Download failed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Step 1: The M2 Parser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing data/QALB-0.9.1-Dec03-2021-SharedTasks/data/2015/dev/QALB-2015-L2-Dev.m2...\n",
                        "Saved 154 pairs to qalb_full_gec.csv\n",
                        "Processing data/QALB-0.9.1-Dec03-2021-SharedTasks/data/2014/dev/QALB-2014-L1-Dev.m2...\n",
                        "Saved 1017 pairs to qalb_full_gec.csv\n"
                    ]
                }
            ],
            "source": [
                "import csv\n",
                "\n",
                "def parse_m2_and_generate_csv(m2_path, output_csv_path):\n",
                "    print(f\"Processing {m2_path}...\")\n",
                "    if not os.path.exists(m2_path):\n",
                "        print(f\"File not found: {m2_path}\")\n",
                "        return\n",
                "\n",
                "    sentences = []\n",
                "    with open(m2_path, 'r', encoding='utf-8') as f:\n",
                "        m2_data = f.read().strip().split(\"\\n\\n\")\n",
                "\n",
                "    processed_data = []\n",
                "\n",
                "    for entry in m2_data:\n",
                "        lines = entry.split(\"\\n\")\n",
                "        if not lines:\n",
                "            continue\n",
                "        \n",
                "        # The first line starts with 'S' and contains the original sentence (tokenized)\n",
                "        source_line = lines[0]\n",
                "        if not source_line.startswith(\"S \"):\n",
                "            continue\n",
                "            \n",
                "        original_tokens = source_line[2:].split()\n",
                "        edits = []\n",
                "        \n",
                "        # Subsequent lines start with 'A' and contain edits\n",
                "        for line in lines[1:]:\n",
                "            if line.startswith(\"A \"):\n",
                "                parts = line[2:].split(\"||\")\n",
                "                # Format: A start_off end_off||type||correction||... \n",
                "                span = parts[0].split()\n",
                "                start_off = int(span[0])\n",
                "                end_off = int(span[1])\n",
                "                correction = parts[2]\n",
                "                edits.append((start_off, end_off, correction))\n",
                "        \n",
                "        # Critical Reversal Logic: Sort edits by start_off in descending order\n",
                "        # This prevents index shifting when modifying the token list\n",
                "        edits.sort(key=lambda x: x[0], reverse=True)\n",
                "        \n",
                "        corrected_tokens = list(original_tokens)\n",
                "        for start, end, subst in edits:\n",
                "            # Python slice replacement: list[start:end] = [new_tokens]\n",
                "            # Determine if replacement is empty (deletion) or has content\n",
                "            if subst == \"-NONE-\":\n",
                "                replacement = []\n",
                "            else:\n",
                "                replacement = subst.split()\n",
                "            \n",
                "            corrected_tokens[start:end] = replacement\n",
                "            \n",
                "        original_sent = \" \".join(original_tokens)\n",
                "        corrected_sent = \" \".join(corrected_tokens)\n",
                "        \n",
                "        processed_data.append([original_sent, corrected_sent])\n",
                "\n",
                "    # Save to CSV\n",
                "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
                "        writer = csv.writer(f)\n",
                "        writer.writerow([\"incorrect\", \"correct\"])\n",
                "        writer.writerows(processed_data)\n",
                "    \n",
                "    print(f\"Saved {len(processed_data)} pairs to {output_csv_path}\")\n",
                "\n",
                "# Example usage (adjust paths after extraction)\n",
                "# Find M2 files in data folder\n",
                "for root, dirs, files in os.walk(\"data\"):\n",
                "    for file in files:\n",
                "        if file.endswith(\".m2\") and \"Dev\" in file:\n",
                "             parse_m2_and_generate_csv(os.path.join(root, file), \"qalb_full_gec.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Step 2: Model Training (AraT5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "51f48aea72be41a9a3c05fd64d0d9e8a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a6312af989004854a90f04d19b7cac28",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/915 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9c99a3f530714221a3742caab6d00881",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/102 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipython-input-3656779288.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer = Seq2SeqTrainer(\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "from datasets import load_dataset\n",
                "import pyarabic.araby as araby\n",
                "\n",
                "def run_training_step():\n",
                "    if not os.path.exists('qalb_full_gec.csv'):\n",
                "        print(\"Training data 'qalb_full_gec.csv' not found. Please run Step 1 Parser first.\")\n",
                "        return\n",
                "\n",
                "    # --- Data Loading ---\n",
                "    # Load the CSV generated in Step 1\n",
                "    dataset = load_dataset('csv', data_files='qalb_full_gec.csv')\n",
                "    \n",
                "    # Split into train/validation (simple split for demo)\n",
                "    dataset = dataset['train'].train_test_split(test_size=0.1)\n",
                "    \n",
                "    # UPDATED: Correct model name\n",
                "    model_name = \"UBC-NLP/AraT5v2-base-1024\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "    # --- Preprocessing ---\n",
                "    prefix = \"gec_arabic: \"\n",
                "    max_input_length = 128\n",
                "    max_target_length = 128\n",
                "\n",
                "    def preprocess_function(examples):\n",
                "        inputs = [prefix + araby.normalize_hamza(ex if ex else \"\") for ex in examples[\"incorrect\"]]\n",
                "        targets = [araby.normalize_hamza(ex if ex else \"\") for ex in examples[\"correct\"]]\n",
                "        \n",
                "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
                "        # Setup the tokenizer for targets\n",
                "        with tokenizer.as_target_tokenizer():\n",
                "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
                "\n",
                "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "        return model_inputs\n",
                "\n",
                "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
                "\n",
                "    # --- Training Config ---\n",
                "    # Updated for speed in demo: 3 epochs\n",
                "    batch_size = 16\n",
                "    args = Seq2SeqTrainingArguments(\n",
                "        \"arat5-gec-checkpoints\",\n",
                "        eval_strategy = \"steps\",\n",
                "        learning_rate=3e-5,\n",
                "        per_device_train_batch_size=batch_size,\n",
                "        per_device_eval_batch_size=batch_size,\n",
                "        weight_decay=0.01,\n",
                "        save_total_limit=3,\n",
                "        num_train_epochs=3,\n",
                "        predict_with_generate=True,\n",
                "        fp16=True, # faster training on GPU\n",
                "        push_to_hub=False,\n",
                "    )\n",
                "\n",
                "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "    # --- Metric (Simplified GLEU/CER placeholder) ---\n",
                "    def compute_metrics(eval_preds):\n",
                "        import numpy as np\n",
                "        preds, labels = eval_preds\n",
                "        # simple decoding for sanity check\n",
                "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "        # Replace -100 in the labels as we can't decode them.\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "        \n",
                "        return {\"sample_pred\": decoded_preds[0], \"sample_label\": decoded_labels[0]}\n",
                "\n",
                "    trainer = Seq2SeqTrainer(\n",
                "        model,\n",
                "        args,\n",
                "        train_dataset=tokenized_datasets[\"train\"],\n",
                "        eval_dataset=tokenized_datasets[\"test\"],\n",
                "        data_collator=data_collator,\n",
                "        tokenizer=tokenizer,\n",
                "        compute_metrics=compute_metrics\n",
                "    )\n",
                "\n",
                "    trainer.train()\n",
                "    \n",
                "    # Save the final model\n",
                "    model.save_pretrained(\"arat5-gec-finetuned\")\n",
                "    tokenizer.save_pretrained(\"arat5-gec-finetuned\")\n",
                "\n",
                "# Uncomment to run training\n",
                "run_training_step()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Step 3: Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_inference(input_sentence):\n",
                "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "    import torch\n",
                "\n",
                "    model_path = \"arat5-gec-finetuned\"\n",
                "    \n",
                "    # Fallback if model isn't trained yet for testing this cell\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
                "        print(\"Loaded Fine-tuned Model\")\n",
                "    except:\n",
                "        print(\"Finetuned model not found, loading base model for demo...\")\n",
                "        # UPDATED: Correct model name\n",
                "        model_name = \"UBC-NLP/AraT5v2-base-1024\"\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "    \n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    model = model.to(device)\n",
                "\n",
                "    # Preprocessing\n",
                "    prefix = \"gec_arabic: \"\n",
                "    text = prefix + input_sentence\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
                "\n",
                "    # Generation\n",
                "    outputs = model.generate(\n",
                "        inputs[\"input_ids\"],\n",
                "        max_length=128,\n",
                "        num_beams=5,\n",
                "        early_stopping=True,\n",
                "        no_repeat_ngram_size=2\n",
                "    )\n",
                "\n",
                "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    return corrected_sentence\n",
                "\n",
                "# Example Output Test\n",
                "test_sentence = \"ذهب الولد الى مدرسة\"\n",
                "print(f\"Original: {test_sentence}\")\n",
                "print(f\"Corrected: {run_inference(test_sentence)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
